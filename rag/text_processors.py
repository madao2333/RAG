import re
import numpy as np
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering
import jieba
import jieba.posseg as pseg
from abc import ABC, abstractmethod

class BaseTextProcessor(ABC):
    """æ–‡æœ¬å¤„ç†å™¨åŸºç±»"""
    
    @abstractmethod
    def clean_text(self, content: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        pass
    
    @abstractmethod
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict[str, Any]]:
        """åˆ†å—æ–‡æœ¬"""
        pass

class GenericProcessor(BaseTextProcessor):
    """é€šç”¨æ–‡æœ¬å¤„ç†å™¨ - ä¼ ç»Ÿåˆ†å—æ–¹æ³•"""
    
    def clean_text(self, content: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å¸¸è§çš„æ— å…³å†…å®¹
        content = re.sub(r'ISBN.*?\n', '', content)
        content = re.sub(r'ç‰ˆæƒæ‰€æœ‰.*?\n', '', content)
        # æ¸…ç†å¤šä½™çš„ç©ºç™½
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = re.sub(r'[ \t]+', ' ', content)
        return content
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿåˆ†å—æ–¹æ³•"""
        chunks = []
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        
        current_chunk = ""
        current_size = 0
        chunk_index = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            para_len = len(para)
            
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šå·²æœ‰å†…å®¹è¶…è¿‡å—å¤§å°ï¼Œåˆ™ä¿å­˜å½“å‰å—
            if current_size + para_len > chunk_size and current_size > 0:
                chunks.append({
                    'text': current_chunk,
                    'metadata': {
                        'source': 'åŒ»å­¦æ–‡çŒ®',
                        'chunk_index': chunk_index,
                        'method': 'traditional'
                    }
                })
                chunk_index += 1
                
                # ä¿ç•™é‡å éƒ¨åˆ†
                overlap_text = current_chunk[-overlap:] if overlap > 0 else ""
                current_chunk = overlap_text + para
                current_size = len(current_chunk)
            else:
                # å¦åˆ™å°†æ®µè½æ·»åŠ åˆ°å½“å‰å—
                if current_size > 0:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
                current_size = len(current_chunk)
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_size > 0:
            chunks.append({
                'text': current_chunk,
                'metadata': {
                    'source': 'åŒ»å­¦æ–‡çŒ®',
                    'chunk_index': chunk_index,
                    'method': 'traditional'
                }
            })
            
        return chunks

class SemanticGenericProcessor(BaseTextProcessor):
    """æ”¯æŒè¯­ä¹‰åˆ†å—çš„é€šç”¨å¤„ç†å™¨"""
    
    def __init__(self, model_path: str = "../models/text2vec-base-chinese"):
        """
        åˆå§‹åŒ–è¯­ä¹‰å¤„ç†å™¨
        
        Args:
            model_path: å‘é‡æ¨¡å‹è·¯å¾„
        """
        try:
            self.embedding_model = SentenceTransformer(model_path)
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}, å°†ä½¿ç”¨ä¼ ç»Ÿåˆ†å—")
            self.embedding_model = None
            
        self.similarity_threshold = 0.6  # ğŸ”¥ é™ä½é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦åˆå¹¶
        self.min_chunk_size = 100  # ğŸ”¥ é™ä½æœ€å°å—å¤§å°ï¼Œå‡å°‘å†…å®¹ä¸¢å¤±
        self.max_chunk_size = 1000
        
    def clean_text(self, content: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å¸¸è§çš„æ— å…³å†…å®¹
        content = re.sub(r'ISBN.*?\n', '', content)
        content = re.sub(r'ç‰ˆæƒæ‰€æœ‰.*?\n', '', content)
        content = re.sub(r'ç›®\s*å½•', 'ç›®å½•', content)
        # æ¸…ç†å¤šä½™çš„ç©ºç™½
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = re.sub(r'[ \t]+', ' ', content)
        return content
    
    def chunk_text(self, text: str, chunk_size: int = 800, overlap: int = 100) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰åˆ†å—ä¸»å‡½æ•° - ç§»é™¤ç»“æ„åŒ–åˆ†å—ï¼Œç›´æ¥ä½¿ç”¨è¯­ä¹‰åˆ†å—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: ç›®æ ‡åˆ†å—å¤§å°
            overlap: é‡å å¤§å°ï¼ˆè¯­ä¹‰åˆ†å—ä¸­ä½œä¸ºå‚è€ƒï¼‰
            
        Returns:
            List[Dict]: åˆ†å—ç»“æœ
        """
        self.max_chunk_size = chunk_size
        
        # ğŸ”¥ å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†å—
        if self.embedding_model is None:
            print("æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†å—")
            return self._fallback_traditional_chunking(text)
        
        try:
            # ğŸ”¥ ç›´æ¥è¿›è¡Œè¯­ä¹‰åˆ†å—ï¼Œä¸å†ä½¿ç”¨ç»“æ„åŒ–åˆ†å—
            print("å¼€å§‹è¯­ä¹‰åˆ†å—...")
            semantic_chunks = self._semantic_chunking(text)
            
            # éªŒè¯åˆ†å—ç»“æœ
            if not semantic_chunks:
                print("è¯­ä¹‰åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†å—")
                return self._fallback_traditional_chunking(text)
            
            # ğŸ”¥ éªŒè¯å†…å®¹å®Œæ•´æ€§
            if self._verify_content_completeness(text, semantic_chunks):
                print(f"âœ… è¯­ä¹‰åˆ†å—æˆåŠŸï¼Œç”Ÿæˆ {len(semantic_chunks)} ä¸ªchunks")
                return semantic_chunks
            else:
                print("âš ï¸ è¯­ä¹‰åˆ†å—å†…å®¹ä¸å®Œæ•´ï¼Œä½¿ç”¨ä¼ ç»Ÿåˆ†å—")
                return self._fallback_traditional_chunking(text)
                
        except Exception as e:
            print(f"è¯­ä¹‰åˆ†å—å¤±è´¥: {e}, ä½¿ç”¨ä¼ ç»Ÿåˆ†å—")
            return self._fallback_traditional_chunking(text)

    def _verify_content_completeness(self, original_text: str, chunks: List[Dict[str, Any]]) -> bool:
        """éªŒè¯åˆ†å—åå†…å®¹çš„å®Œæ•´æ€§"""
        try:
            # åˆå¹¶æ‰€æœ‰chunksçš„æ–‡æœ¬
            combined_text = ''.join(chunk['text'] for chunk in chunks)
            
            # ç§»é™¤ç©ºç™½å­—ç¬¦åæ¯”è¾ƒ
            original_clean = re.sub(r'\s+', '', original_text)
            combined_clean = re.sub(r'\s+', '', combined_text)
            
            # è®¡ç®—ä¿ç•™ç‡
            retention_rate = len(combined_clean) / len(original_clean) if original_clean else 0
            
            print(f"å†…å®¹ä¿ç•™ç‡: {retention_rate:.2%}")
            
            if retention_rate < 0.95:  # å¦‚æœä¸¢å¤±è¶…è¿‡5%
                print(f"âš ï¸ å†…å®¹ä¸¢å¤±è¿‡å¤šï¼Œä¿ç•™ç‡ä»… {retention_rate:.2%}")
                return False
            
            return True
            
        except Exception as e:
            print(f"å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
            return False

    def _fallback_traditional_chunking(self, text: str) -> List[Dict[str, Any]]:
        """å¤‡ç”¨ä¼ ç»Ÿåˆ†å—æ–¹æ³•"""
        chunks = []
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not paragraphs:
            # å¦‚æœæ²¡æœ‰æ®µè½ï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ†å—
            for i in range(0, len(text), self.max_chunk_size - 100):
                chunk_text = text[i:i + self.max_chunk_size]
                if chunk_text.strip():
                    chunks.append({
                        'text': chunk_text.strip(),
                        'metadata': {
                            'source': 'åŒ»å­¦æ–‡çŒ®',
                            'type': 'semantic',
                            'chunk_index': len(chunks),
                            'total_chunks': 'unknown',
                            'semantic_method': 'fallback_fixed'
                        }
                    })
        else:
            # æŒ‰æ®µè½åˆ†å—
            current_chunk = ""
            chunk_index = 0
            
            for para in paragraphs:
                if len(current_chunk + "\n\n" + para) > self.max_chunk_size and current_chunk:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'metadata': {
                            'source': 'åŒ»å­¦æ–‡çŒ®',
                            'type': 'semantic',
                            'chunk_index': chunk_index,
                            'total_chunks': 'unknown',
                            'semantic_method': 'fallback_paragraph'
                        }
                    })
                    chunk_index += 1
                    current_chunk = para
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + para
                    else:
                        current_chunk = para
            
            # æ·»åŠ æœ€åä¸€ä¸ªchunk
            if current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': {
                        'source': 'åŒ»å­¦æ–‡çŒ®',
                        'type': 'semantic',
                        'chunk_index': chunk_index,
                        'total_chunks': chunk_index + 1,
                        'semantic_method': 'fallback_paragraph'
                    }
                })
        
        # æ›´æ–°æ€»æ•°
        for chunk in chunks:
            chunk['metadata']['total_chunks'] = len(chunks)
        
        print(f"ä¼ ç»Ÿå¤‡ç”¨åˆ†å—ç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        return chunks
    
    def _structural_chunking(self, text: str) -> List[Dict[str, Any]]:
        """ç»“æ„åŒ–åˆ†å— - å…ˆæŒ‰ç« èŠ‚ç­‰ç»“æ„åˆ†å‰²"""
        chunks = []
        
        # æŒ‰ç« èŠ‚åˆ†å‰²
        chapter_pattern = r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚ç¯‡][^\n]*'
        chapter_matches = list(re.finditer(chapter_pattern, text))
        
        if len(chapter_matches) < 2:  # ç« èŠ‚å¤ªå°‘ï¼Œä¸è¿›è¡Œç»“æ„åŒ–åˆ†å—
            return []
        
        for i, match in enumerate(chapter_matches):
            chapter_start = match.start()
            chapter_end = chapter_matches[i + 1].start() if i + 1 < len(chapter_matches) else len(text)
            
            chapter_text = text[chapter_start:chapter_end].strip()
            chapter_title = match.group().strip()
            
            # æå–ç« èŠ‚ä¿¡æ¯
            chapter_match = re.match(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)[ç« èŠ‚ç¯‡]\s*(.+)', chapter_title)
            if chapter_match:
                chapter_num = chapter_match.group(1)
                chapter_name = chapter_match.group(2).strip()
            else:
                chapter_num = str(i + 1)
                chapter_name = chapter_title
            
            chunks.append({
                'text': chapter_text,
                'metadata': {
                    'source': 'åŒ»å­¦æ–‡çŒ®',
                    'chapter_num': chapter_num,
                    'chapter_title': chapter_name,
                    'type': 'structural_chapter'
                }
            })
        
        return chunks
    
    def _semantic_chunking(self, text: str, base_metadata: Dict = None) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰åˆ†å—æ ¸å¿ƒæ–¹æ³• - å¥å­çº§åˆ†å—ï¼ˆé¿å…ä¿¡æ¯ä¸¢å¤±ç‰ˆï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            base_metadata: åŸºç¡€å…ƒæ•°æ®
            
        Returns:
            List[Dict]: è¯­ä¹‰åˆ†å—ç»“æœ
        """
        if base_metadata is None:
            base_metadata = {'source': 'åŒ»å­¦æ–‡çŒ®', 'type': 'semantic'}
        
        # ğŸ”¥ æ”¹å›å¥å­çº§åˆ†å—ï¼Œä½†å¢å¼ºåˆ†å‰²é€»è¾‘
        sentences = self._safe_split_into_sentences(text)
        
        if len(sentences) <= 3:  # å¥å­å¤ªå°‘ï¼Œç›´æ¥è¿”å›
            return [{
                'text': text,
                'metadata': {
                    **base_metadata,
                    'chunk_index': 0,
                    'total_chunks': 1,
                    'semantic_method': 'too_short'
                }
            }]
        
        print(f"æ€»å…± {len(sentences)} ä¸ªå¥å­")
        
        # ğŸ”¥ éªŒè¯å¥å­åˆ†å‰²çš„å®Œæ•´æ€§
        combined_sentences = ''.join(sentences)
        original_no_space = re.sub(r'\s+', '', text)
        combined_no_space = re.sub(r'\s+', '', combined_sentences)
        
        if len(combined_no_space) < len(original_no_space) * 0.95:  # ä¸¢å¤±è¶…è¿‡5%
            print(f"âš ï¸ å¥å­åˆ†å‰²å¯èƒ½ä¸¢å¤±å†…å®¹ï¼Œå›é€€åˆ°æ®µè½åˆ†å—")
            return self._simple_paragraph_grouping(text.split('\n\n'), base_metadata)
        
        # ä½¿ç”¨å¥å­è¿›è¡Œè¯­ä¹‰åˆ†æ
        if len(sentences) > 30:  # å¥å­è¾ƒå¤šæ—¶æ‰ä½¿ç”¨è¯­ä¹‰èšç±»
            try:
                print(f"è®¡ç®— {len(sentences)} ä¸ªå¥å­çš„å‘é‡...")
                sentence_embeddings = self.embedding_model.encode(sentences)
                
                # ğŸ”¥ å®‰å…¨çš„è¯­ä¹‰èšç±»
                semantic_groups = self._safe_semantic_clustering(sentences, sentence_embeddings)
                
                # ğŸ”¥ éªŒè¯èšç±»ç»“æœçš„å®Œæ•´æ€§
                if not self._validate_clustering_completeness(semantic_groups, len(sentences)):
                    print("èšç±»ç»“æœä¸å®Œæ•´ï¼Œä½¿ç”¨é¡ºåºåˆ†ç»„")
                    semantic_groups = self._sequential_sentence_grouping(sentences)
                
                # ç”Ÿæˆåˆ†å—
                chunks = self._create_sentence_chunks(semantic_groups, sentences, base_metadata)
                
                return chunks
                
            except Exception as e:
                print(f"å¥å­è¯­ä¹‰èšç±»å¤±è´¥: {e}, ä½¿ç”¨é¡ºåºåˆ†ç»„")
                return self._sequential_sentence_chunking(sentences, base_metadata)
        else:
            # å¥å­è¾ƒå°‘æ—¶ç›´æ¥é¡ºåºåˆ†ç»„
            return self._sequential_sentence_chunking(sentences, base_metadata)
    
    def _safe_split_into_sentences(self, text, language='zh'):
        """
        å®‰å…¨åœ°å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­ï¼Œä¼˜å…ˆä½¿ç”¨ä¸­æ–‡å‹å¥½çš„åº“
        """
        if not text or not text.strip():
            print("âš ï¸  è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦")
            return []
        
        text = text.strip()
        print(f"\nğŸ” å¼€å§‹åˆ†å‰²æ–‡æœ¬ (è¯­è¨€: {language})")
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"ğŸ“„ åŸå§‹æ–‡æœ¬å‰100å­—ç¬¦: {text[:100]}{'...' if len(text) > 100 else ''}")
        
        sentences = []
        
        try:
            if language == 'zh':
                # ğŸ”¥ ä¼˜å…ˆå°è¯•ä¸­æ–‡å‹å¥½çš„åˆ†å‰²æ–¹æ³•
                
                # æ–¹æ³•1: spaCyä¸­æ–‡æ¨¡å‹ (æœ€æ¨è)
                try:
                    import spacy
                    nlp = spacy.load("zh_core_web_sm")
                    doc = nlp(text)
                    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
                    print("âœ… ä½¿ç”¨ spaCy ä¸­æ–‡æ¨¡å‹åˆ†å‰²å®Œæˆ")
                    
                except (ImportError, OSError):
                    print("âš ï¸  spaCyä¸­æ–‡æ¨¡å‹æœªå®‰è£…")
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬åˆ†å‰²è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # ä½¿ç”¨æœ€åŸºç¡€çš„åˆ†å‰²æ–¹æ³•ä½œä¸ºæœ€åå¤‡é€‰
            sentences = [s.strip() for s in text.split('\n') if s.strip()]
            if not sentences:
                sentences = [text]
            print("ğŸ”„ ä½¿ç”¨åŸºç¡€æ¢è¡Œç¬¦åˆ†å‰²ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
        
        # æ¸…ç†å’Œè¿‡æ»¤å¥å­
        print("\nğŸ§¹ å¼€å§‹æ¸…ç†å¥å­...")
        original_count = len(sentences)
        
        cleaned_sentences = []
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if sentence and len(sentence) > 1:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„å¥å­
                cleaned_sentences.append(sentence)
                print(f"  âœ“ å¥å­ {i+1}: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
            else:
                print(f"  âœ— è·³è¿‡å¥å­ {i+1}: '{sentence}' (å¤ªçŸ­æˆ–ä¸ºç©º)")
        
        sentences = cleaned_sentences
        
        # å¦‚æœæ²¡æœ‰å¥å­ï¼Œå°è¯•æ›´å®½æ¾çš„åˆ†å‰²
        if not sentences:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå¥å­ï¼Œå°è¯•æ›´å®½æ¾çš„åˆ†å‰²...")
            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²
            sentences = [s.strip() for s in text.split('\n') if s.strip()]
            if sentences:
                print(f"âœ… æŒ‰æ¢è¡Œç¬¦åˆ†å‰²å¾—åˆ° {len(sentences)} ä¸ªå¥å­")
            else:
                # æœ€åæ‰‹æ®µï¼šæ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå¥å­
                sentences = [text]
                print("âš ï¸  ä½¿ç”¨æ•´ä¸ªæ–‡æœ¬ä½œä¸ºå•ä¸ªå¥å­")
        
        print(f"\nğŸ“Š åˆ†å‰²ç»Ÿè®¡:")
        print(f"  - æ¸…ç†åå¥å­æ•°: {len(sentences)}")
        
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: å…± {len(sentences)} ä¸ªå¥å­")
        for i, sentence in enumerate(sentences[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªå¥å­
            print(f"  ğŸ“ å¥å­ {i+1} ({len(sentence)} å­—ç¬¦): {sentence}")
        
        if len(sentences) > 5:
            print(f"  ... çœç•¥å…¶ä½™ {len(sentences) - 5} ä¸ªå¥å­")
        
        print("=" * 60)
        return sentences

    def _safe_semantic_clustering(self, sentences: List[str], embeddings: np.ndarray) -> List[List[int]]:
        """å®‰å…¨çš„è¯­ä¹‰èšç±» - ç¡®ä¿æ‰€æœ‰å¥å­éƒ½è¢«åŒ…å«"""
        try:
            if len(sentences) <= 3:
                return [list(range(len(sentences)))]
            
            # ğŸ”¥ ä½¿ç”¨æ»‘åŠ¨çª—å£çš„è´ªå¿ƒèšç±»ï¼Œç¡®ä¿æ‰€æœ‰å¥å­éƒ½è¢«å¤„ç†
            groups = []
            used = [False] * len(sentences)  # ä½¿ç”¨å¸ƒå°”æ•°ç»„è·Ÿè¸ª
            
            for i in range(len(sentences)):
                if used[i]:
                    continue
                
                current_group = [i]
                used[i] = True
                current_text_length = len(sentences[i])
                
                # ğŸ”¥ å‘å‰å’Œå‘åæœç´¢ç›¸ä¼¼å¥å­
                search_range = min(10, len(sentences))  # é™åˆ¶æœç´¢èŒƒå›´
                
                for offset in range(1, search_range):
                    # å‘åæœç´¢
                    j = i + offset
                    if j < len(sentences) and not used[j]:
                        similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
                        if (similarity > self.similarity_threshold and 
                            current_text_length + len(sentences[j]) <= self.max_chunk_size):
                            current_group.append(j)
                            used[j] = True
                            current_text_length += len(sentences[j])
                    
                    # å‘å‰æœç´¢ï¼ˆå¦‚æœi>0ï¼‰
                    k = i - offset
                    if k >= 0 and not used[k]:
                        similarity = cosine_similarity([embeddings[i]], [embeddings[k]])[0][0]
                        if (similarity > self.similarity_threshold and 
                            current_text_length + len(sentences[k]) <= self.max_chunk_size):
                            current_group.append(k)
                            used[k] = True
                            current_text_length += len(sentences[k])
                
                if current_group:
                    groups.append(sorted(current_group))  # ä¿æŒå¥å­é¡ºåº
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„å¥å­
            all_used_indices = set()
            for group in groups:
                all_used_indices.update(group)
            
            missing_indices = set(range(len(sentences))) - all_used_indices
            if missing_indices:
                print(f"å‘ç° {len(missing_indices)} ä¸ªæœªåˆ†ç»„çš„å¥å­ï¼Œæ·»åŠ åˆ°ç‹¬ç«‹ç»„")
                for idx in sorted(missing_indices):
                    groups.append([idx])
            
            return groups
            
        except Exception as e:
            print(f"è¯­ä¹‰èšç±»å¤±è´¥: {e}, ä½¿ç”¨é¡ºåºåˆ†ç»„")
            return self._sequential_sentence_grouping(sentences)

    def _sequential_sentence_grouping(self, sentences: List[str]) -> List[List[int]]:
        """é¡ºåºå¥å­åˆ†ç»„ - ç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•å¥å­"""
        groups = []
        current_group = []
        current_length = 0
        
        for i, sentence in enumerate(sentences):
            sentence_length = len(sentence)
            
            if current_length + sentence_length > self.max_chunk_size and current_group:
                groups.append(current_group)
                current_group = [i]
                current_length = sentence_length
            else:
                current_group.append(i)
                current_length += sentence_length
        
        if current_group:
            groups.append(current_group)
        
        return groups

    def _validate_clustering_completeness(self, groups: List[List[int]], total_sentences: int) -> bool:
        """éªŒè¯èšç±»ç»“æœçš„å®Œæ•´æ€§"""
        all_indices = set()
        for group in groups:
            all_indices.update(group)
        
        return len(all_indices) == total_sentences

    def _sequential_sentence_chunking(self, sentences: List[str], base_metadata: Dict) -> List[Dict[str, Any]]:
        """é¡ºåºå¥å­åˆ†å— - ç¡®ä¿ä¸ä¸¢å¤±å†…å®¹"""
        chunks = []
        current_chunk_sentences = []
        current_length = 0
        chunk_index = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > self.max_chunk_size and current_chunk_sentences:
                # ä¿å­˜å½“å‰chunk
                chunk_text = ''.join(current_chunk_sentences)
                keywords = self._simple_extract_keywords(chunk_text)
                
                chunks.append({
                    'text': chunk_text.strip(),
                    'metadata': {
                        **base_metadata,
                        'chunk_index': chunk_index,
                        'total_chunks': 'unknown',
                        'semantic_method': 'sequential_sentences',
                        'sentence_count': len(current_chunk_sentences),
                        'keywords': keywords
                    }
                })
                
                chunk_index += 1
                current_chunk_sentences = [sentence]
                current_length = sentence_length
            else:
                current_chunk_sentences.append(sentence)
                current_length += sentence_length
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk_sentences:
            chunk_text = ''.join(current_chunk_sentences)
            keywords = self._simple_extract_keywords(chunk_text)
            
            chunks.append({
                'text': chunk_text.strip(),
                'metadata': {
                    **base_metadata,
                    'chunk_index': chunk_index,
                    'total_chunks': chunk_index + 1,
                    'semantic_method': 'sequential_sentences',
                    'sentence_count': len(current_chunk_sentences),
                    'keywords': keywords
                }
            })
        
        # æ›´æ–°æ€»æ•°
        for chunk in chunks:
            chunk['metadata']['total_chunks'] = len(chunks)
        
        print(f"é¡ºåºå¥å­åˆ†å—ç”Ÿæˆ {len(chunks)} ä¸ªchunks")
        return chunks

    def _create_sentence_chunks(self, groups: List[List[int]], sentences: List[str], base_metadata: Dict) -> List[Dict[str, Any]]:
        """ä»å¥å­ç»„åˆ›å»ºæœ€ç»ˆchunks - ç¡®ä¿ä¸ä¸¢å¤±å†…å®¹"""
        chunks = []
        
        for i, group in enumerate(groups):
            if not group:  # è·³è¿‡ç©ºç»„
                continue
            
            # ğŸ”¥ æŒ‰ç´¢å¼•é¡ºåºç»„åˆå¥å­ï¼Œä¿æŒåŸæœ‰é¡ºåº
            group_sentences = [sentences[idx] for idx in sorted(group)]
            chunk_text = ''.join(group_sentences)  # ä¸æ·»åŠ é¢å¤–æ ‡ç‚¹ï¼Œä¿æŒåŸæ ·
            
            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            if not chunk_text.strip():
                continue
            
            keywords = self._simple_extract_keywords(chunk_text)
            
            chunks.append({
                'text': chunk_text.strip(),
                'metadata': {
                    **base_metadata,
                    'chunk_index': i,
                    'total_chunks': len(groups),
                    'semantic_method': 'sentence_clustering',
                    'sentence_count': len(group),
                    'keywords': keywords,
                    'semantic_coherence': self._safe_calculate_coherence(group_sentences)
                }
            })
        
        return chunks

    def _safe_calculate_coherence(self, sentences: List[str]) -> float:
        """å®‰å…¨è®¡ç®—è¯­ä¹‰è¿è´¯æ€§"""
        try:
            if len(sentences) <= 1:
                return 1.0
            
            if self.embedding_model is None:
                return 1.0
            
            # è®¡ç®—å¥å­å‘é‡
            embeddings = self.embedding_model.encode(sentences)
            
            # è®¡ç®—ç›¸é‚»å¥å­çš„å¹³å‡ç›¸ä¼¼åº¦
            similarities = []
            for i in range(len(embeddings) - 1):
                similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]
                similarities.append(float(similarity))  # ğŸ”¥ ç«‹å³è½¬æ¢
            
            return float(np.mean(similarities)) if similarities else 1.0
            
        except Exception as e:
            print(f"è¿è´¯æ€§è®¡ç®—å¤±è´¥: {e}")
            return 1.0
    
    # ğŸ”¥ ä¿ç•™å…¶ä»–åŸæœ‰æ–¹æ³•ä½†ç®€åŒ–é€»è¾‘
    def _semantic_clustering(self, sentences: List[str], embeddings: np.ndarray) -> List[List[int]]:
        """è¯­ä¹‰èšç±» - ç®€åŒ–ç‰ˆ"""
        try:
            if len(sentences) <= 3:
                return [list(range(len(sentences)))]
            
            # ä½¿ç”¨æ›´ç®€å•çš„èšç±»ç­–ç•¥
            groups = []
            used = set()
            
            for i in range(len(sentences)):
                if i in used:
                    continue
                
                current_group = [i]
                used.add(i)
                
                # åªä¸ç›¸é‚»çš„å¥å­æ¯”è¾ƒ
                for j in range(i + 1, min(i + 5, len(sentences))):  # é™åˆ¶æœç´¢èŒƒå›´
                    if j in used:
                        continue
                        
                    similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
                    if similarity > self.similarity_threshold:
                        current_group.append(j)
                        used.add(j)
                
                groups.append(current_group)
            return groups
        except Exception as e:
            print(f"èšç±»å¤±è´¥: {e}")
            # è¿”å›ç®€å•åˆ†ç»„
            return [[i] for i in range(len(sentences))]
    
    def _merge_adjacent_groups(self, groups: List[List[int]], embeddings: np.ndarray) -> List[List[int]]:
        """åˆå¹¶ç›¸é‚»ç»„ - ç®€åŒ–ç‰ˆ"""
        try:
            return groups  # ğŸ”¥ æš‚æ—¶ä¸åˆå¹¶ï¼Œé¿å…å¤æ‚åº¦
        except Exception as e:
            print(f"åˆå¹¶å¤±è´¥: {e}")
            return groups
    
    def _create_final_chunks(self, groups: List[List[int]], sentences: List[str], base_metadata: Dict) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæœ€ç»ˆåˆ†å— - ç¡®ä¿ä¸ä¸¢å¤±å†…å®¹"""
        chunks = []
        
        for i, group in enumerate(groups):
            # æŒ‰é¡ºåºç»„åˆå¥å­
            group_sentences = [sentences[idx] for idx in sorted(group)]
            chunk_text = 'ã€‚'.join(group_sentences) + 'ã€‚'
            
            # ğŸ”¥ ç§»é™¤å°å—è¿‡æ»¤ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½ä¿å­˜
            keywords = self._extract_keywords(group_sentences)
            
            chunks.append({
                'text': chunk_text,
                'metadata': {
                    **base_metadata,
                    'chunk_index': i,
                    'total_chunks': len(groups),
                    'semantic_method': 'clustering',
                    'sentence_count': len(group),
                    'keywords': keywords,
                    'semantic_coherence': self._calculate_coherence(group_sentences)
                }
            })
        
        return chunks
    
    def _simple_extract_keywords(self, text: str) -> List[str]:
            """ç®€åŒ–çš„å…³é”®è¯æå– - å¢åŠ å®¹é”™æ€§"""
            try:
                # å°è¯•ä½¿ç”¨jieba
                words = jieba.cut(text)
                keywords = []
                for word in words:
                    if len(word) >= 2 and len(word) <= 4:
                        if re.match(r'[\u4e00-\u9fa5]+', word):  # åªè¦ä¸­æ–‡
                            keywords.append(word)
                
                # å»é‡å¹¶è¿”å›å‰5ä¸ª
                return list(dict.fromkeys(keywords))[:5]
                
            except Exception as e:
                # å¦‚æœjiebaå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
                try:
                    words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
                    return list(dict.fromkeys(words))[:5]
                except:
                    return []
        
    def _simple_paragraph_grouping(self, paragraphs: List[str], base_metadata: Dict) -> List[Dict[str, Any]]:
        """ç®€å•çš„æ®µè½åˆ†ç»„"""
        chunks = []
        current_chunk = ""
        chunk_index = 0
        
        for para in paragraphs:
            if len(current_chunk + "\n\n" + para) > self.max_chunk_size and current_chunk:
                # ä¿å­˜å½“å‰chunk
                keywords = self._simple_extract_keywords(current_chunk)
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': {
                        **base_metadata,
                        'chunk_index': chunk_index,
                        'total_chunks': 'unknown',
                        'semantic_method': 'simple_grouping',
                        'keywords': keywords
                    }
                })
                chunk_index += 1
                current_chunk = para
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk:
            keywords = self._simple_extract_keywords(current_chunk)
            chunks.append({
                'text': current_chunk.strip(),
                'metadata': {
                    **base_metadata,
                    'chunk_index': chunk_index,
                    'total_chunks': chunk_index + 1,
                    'semantic_method': 'simple_grouping',
                    'keywords': keywords
                }
            })
        
        # æ›´æ–°æ€»æ•°
        for chunk in chunks:
            chunk['metadata']['total_chunks'] = len(chunks)
        
        return chunks
class HybridGenericProcessor(BaseTextProcessor):
    """æ··åˆå¤„ç†å™¨ï¼šç»“åˆä¼ ç»Ÿåˆ†å—å’Œè¯­ä¹‰åˆ†å—"""
    
    def __init__(self, model_path: str = "../models/text2vec-base-chinese", use_semantic: bool = True):
        self.use_semantic = use_semantic
        if use_semantic:
            self.semantic_processor = SemanticGenericProcessor(model_path)
        self.traditional_processor = GenericProcessor()
    
    def clean_text(self, content: str) -> str:
        if self.use_semantic:
            return self.semantic_processor.clean_text(content)
        else:
            return self.traditional_processor.clean_text(content)
    
    def chunk_text(self, text: str, chunk_size: int = 800, overlap: int = 100) -> List[Dict[str, Any]]:
        if self.use_semantic and len(text) > 1000:  # åªå¯¹è¾ƒé•¿æ–‡æœ¬ä½¿ç”¨è¯­ä¹‰åˆ†å—
            print("ä½¿ç”¨è¯­ä¹‰åˆ†å—...")
            return self.semantic_processor.chunk_text(text, chunk_size, overlap)
        else:
            print("ä½¿ç”¨ä¼ ç»Ÿåˆ†å—...")
            return self.traditional_processor.chunk_text(text, chunk_size, overlap)

class ProcessorFactory:
    """å¤„ç†å™¨å·¥å‚"""
    
    @staticmethod
    def get_processor(text_file: str = "", use_semantic: bool = False) -> BaseTextProcessor:
        """ç»Ÿä¸€ä½¿ç”¨è¯­ä¹‰å¢å¼ºçš„é€šç”¨å¤„ç†å™¨"""
        print(f"ä½¿ç”¨é€šç”¨è¯­ä¹‰å¤„ç†å™¨ (æ–‡ä»¶: {text_file})")
        return HybridGenericProcessor(use_semantic=True)