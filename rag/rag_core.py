import re
import json
import numpy as np
import faiss
from pathlib import Path
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any

class RAGCore:
    """RAGç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½ï¼ˆé€šç”¨ï¼‰"""
    
    def __init__(self, model_name: str = '../models/text2vec-base-chinese'):
        # ğŸ”¥ é»˜è®¤ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„ä¸­æ–‡æ¨¡å‹
        self.model_name = model_name
        self.model = None
    
    def create_embeddings(self, chunks: List[Dict[str, Any]]) -> tuple:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥"""
        if self.model is None:
            try:
                print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
                
                # ğŸ”¥ å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹
                if self.model_name.startswith('../models/'):
                    # æœ¬åœ°æ¨¡å‹è·¯å¾„
                    model_path = Path(self.model_name)
                    if model_path.exists():
                        self.model = SentenceTransformer(str(model_path))
                        print(f"âœ“ æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
                    else:
                        print(f"âœ— æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
                else:
                    # åœ¨çº¿æ¨¡å‹
                    self.model = SentenceTransformer(self.model_name)
                    print(f"âœ“ æˆåŠŸåŠ è½½åœ¨çº¿æ¨¡å‹: {self.model_name}")
                
                # ğŸ”¥ æµ‹è¯•æ¨¡å‹
                test_embedding = self.model.encode(["æµ‹è¯•ä¸­æ–‡æ–‡æœ¬"])
                print(f"  æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼Œå‘é‡ç»´åº¦: {test_embedding.shape}")
                
            except Exception as e:
                print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.model.encode(texts, show_progress_bar=True)
        
        return embeddings, self.model
    
    def build_vector_index(self, embeddings: np.ndarray) -> faiss.Index:
        """æ„å»ºFAISSå‘é‡ç´¢å¼•"""
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(embeddings)
        index.add(embeddings.astype('float32'))
        return index
    
    def save_chunks(self, chunks: List[Dict[str, Any]], save_path: str):
        """ä¿å­˜chunksåˆ°æ–‡ä»¶"""
        save_file = Path(save_path)
        save_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(save_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        
        print(f"Chunkså·²ä¿å­˜åˆ°: {save_file}")
        print(f"å…±ä¿å­˜ {len(chunks)} ä¸ªæ–‡æ¡£å—")
    
    def load_chunks(self, load_path: str) -> List[Dict[str, Any]]:
        """ä»æ–‡ä»¶åŠ è½½chunks"""
        load_file = Path(load_path)
        
        if not load_file.exists():
            raise FileNotFoundError(f"Chunksæ–‡ä»¶ä¸å­˜åœ¨: {load_file}")
        
        with open(load_file, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        print(f"ä» {load_file} åŠ è½½äº† {len(chunks)} ä¸ªæ–‡æ¡£å—")
        return chunks
    
    def build_rag_system(self, text_file: str, save_dir: str,
                        save_chunks: bool = True) -> 'UniversalRAG':
        """æ„å»ºRAGç³»ç»Ÿ"""
        from text_processors import ProcessorFactory
        from rag_system import UniversalRAG
        
        print(f"æ„å»ºRAGç³»ç»Ÿ: {text_file}")
        
        # 1. è·å–å¤„ç†å™¨
        processor = ProcessorFactory.get_processor(text_file)
        
        # 2. æ¸…ç†æ–‡æœ¬
        print("1. æ¸…ç†æ–‡æœ¬...")
        cleaned_text = self._clean_common_text(text_file)
        cleaned_text = processor.clean_text(cleaned_text)
        
        # 3. æ–‡æ¡£åˆ†å—
        print("2. æ–‡æ¡£åˆ†å—...")
        chunks = processor.chunk_text(cleaned_text)
        print(f"ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        if not chunks:
            raise ValueError("æœªèƒ½ç”Ÿæˆä»»ä½•æ–‡æ¡£å—ï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ ¼å¼")
        
        # ğŸ”¥ æ–°å¢ï¼šä¿å­˜chunksï¼ˆå¯é€‰ï¼‰
        if save_chunks:
            chunks_file = Path(save_dir) / "chunks_only.json"
            self.save_chunks(chunks, chunks_file)
        
        # 4. ç”Ÿæˆå‘é‡åµŒå…¥
        print("3. ç”Ÿæˆå‘é‡åµŒå…¥...")
        embeddings, model = self.create_embeddings(chunks)
        
        # 5. æ„å»ºå‘é‡ç´¢å¼•
        print("4. æ„å»ºå‘é‡ç´¢å¼•...")
        index = self.build_vector_index(embeddings)
        
        # 6. åˆ›å»ºRAGç³»ç»Ÿ
        print("5. åˆ›å»ºRAGç³»ç»Ÿ...")
        rag_system = UniversalRAG(chunks, embeddings, index, model)
        
        # 7. ä¿å­˜ç³»ç»Ÿ
        print("6. ä¿å­˜ç³»ç»Ÿ...")
        rag_system.save(save_dir)
        
        return rag_system
    
    def build_rag_from_chunks(self, chunks_file: str, save_dir: str) -> 'UniversalRAG':
        """ä»å·²ä¿å­˜çš„chunksæ„å»ºRAGç³»ç»Ÿ"""
        from rag_system import UniversalRAG
        
        print(f"ä»chunksæ–‡ä»¶æ„å»ºRAGç³»ç»Ÿ: {chunks_file}")
        
        # 1. åŠ è½½chunks
        print("1. åŠ è½½chunks...")
        chunks = self.load_chunks(chunks_file)
        
        # 2. ç”Ÿæˆå‘é‡åµŒå…¥
        print("2. ç”Ÿæˆå‘é‡åµŒå…¥...")
        embeddings, model = self.create_embeddings(chunks)
        
        # 3. æ„å»ºå‘é‡ç´¢å¼•
        print("3. æ„å»ºå‘é‡ç´¢å¼•...")
        index = self.build_vector_index(embeddings)
        
        # 4. åˆ›å»ºRAGç³»ç»Ÿ
        print("4. åˆ›å»ºRAGç³»ç»Ÿ...")
        rag_system = UniversalRAG(chunks, embeddings, index, model)
        
        # 5. ä¿å­˜ç³»ç»Ÿ
        print("5. ä¿å­˜ç³»ç»Ÿ...")
        rag_system.save(save_dir)
        
        return rag_system
    
    def _clean_common_text(self, text_file: str) -> str:
        """é€šç”¨æ–‡æœ¬æ¸…ç†"""
        with open(text_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # é€šç”¨æ¸…ç†è§„åˆ™
        # ç§»é™¤é¡µé¢æ ‡è®°
        content = re.sub(r'--- ç¬¬ \d+ é¡µ ---', '', content)
        
        # ç§»é™¤æ–‡ä»¶å¤´ä¿¡æ¯
        content = re.sub(r'# æºæ–‡ä»¶:.*?\n# æ–‡æœ¬é•¿åº¦:.*?\n\n', '', content, flags=re.DOTALL)
        
        return content.strip()